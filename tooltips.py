#You can add tooltips for the parameters of the training process here
tooltips = {
    'name': 'The name of the training process',
    'acronym': 'A short form or abbreviation of the name',
    'folder': 'The directory where the training data or results are stored',
    'type': 'single/leaveoneout: Specifies the type of training process.',
    'originaldataset': 'The path to the original dataset used for training',
    'silentmode': 'integer 1/0: a flag to control the verbosity of the training process. If "1", log is not reproduced on STDIN',
    'percent': 'LIST of two floats: it specifies the percentage of data to be used for training and testing. The remaining will be used for validation',
    '_01ratio': 'Specifies the ratio of class 0 to class 1 in the data',
    'includedspecies': 'LIST of strings: Specifies the species to be included in the training process - "" means all species are included',
    'excludedspecies': 'LIST of strings: Specifies the species to be excluded from the training process - [] means no species are excluded',
    'leaveoneoutspecies': 'LIST of LIST of strings: each list specifies a species or group of species, to be used for testing in the leave-one-out training process',
    'createflag': 'yes/no/ask: a flag to control whether a new dataset (or new embeddings) have to be generated if already present',
    'trainflag': 'yes/no/ask: a flag to control whether model already existing have to be retrained',
    'inferflag': 'yes/no/ask: a flag to control whether an inference has to be repeated in case a file with the results is already present',
    'batch_size': 'LIST: specifies the batch size for the training process',
    'epoch': 'LIST: specifies the number of epochs for the training/fine-tuning process',
    'learning_rate': 'LIST: specifies the learning rate for the training/fine-tuning process',
    'model_name': 'name of the class (in model_util.py) corresponding to the model to be used for training/fine-tuning',
    'annotation': 'LIST: for each label 0/1 in training/fine-tuning, it specifies the type of annotation (manual/automatic) to be used to select data for training/fine-tuning/inference',
    'reviewed': 'LIST: for each label 0/1 in training/fine-tuning, it specifies if to use reviewed proteins or not download or to select data for training/fine-tuning/inference',
    'torchdevice': 'Specifies the hardware to be used for training/fine-tuning/inference MAC: mps, WIN: cpu, if CUDA is available it is automatically selected',
    't5secstructfolder': 'The folder where the T5 model for secondary structure prediction is stored - This model is used to compute embeddings for the sequences in the dataset',
    'folder': 'The folder where the files downloaded from Uniprot are stored and the embeddings will be created',
    'sequencebatchsize': 'The batch size to be used for the sequence embeddings computation - It depends on the system overall computational power and RAM memory. 25 works on MAC M2 with 64GB RAM',
    'go_ids': 'LIST: GO terms to be searched. You will get results of proteins matchin ANY of these GO Terms',
    'go_includedescendants' : 'string "True"/"False": a flag to control whether the descendants of the GO terms have to be included in the search',
    'go_batchsize' : '-1 or integer. If -1, the "stream" REST service is used, otherwise the "search" REST call is used. "search" can be used to download in batches when the connection with UniProt is problematic or slow. In that case, 500 is a good value',
    'go_maxproteinsdownload' : '-1 means NO LIMIT, otherwise the MAX number of TOTAL proteins to be downloaded - The minimum is in any case batchsize.',
    'go_taxonomies' : 'LIST: Taxonomy IDs to be used for the search',
    'go_folder' : 'The folder where the files downloaded from Uniprot are stored and the embeddings will be created',
    'datasetname' : 'The name of the dataset to be created',
    'cachedataset' : 'LIST: a list of dataset.embeddings.csv files that can be used as cache during embeddings computation'
}
